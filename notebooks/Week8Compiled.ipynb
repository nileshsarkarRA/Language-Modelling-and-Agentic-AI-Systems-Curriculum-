{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e943ec0e",
   "metadata": {},
   "source": [
    "## Week 8 Lab Manual\n",
    "### Foundations of Deep Learning & AI Functionality\n",
    "\n",
    "**Instructor Note**: This lab manual provides the aim, code, and explanation for each practical task. Focus on the architectural patterns and the transition from theoretical concepts to functional AI implementations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc7d55",
   "metadata": {},
   "source": [
    "# Week 8: Model Evaluation & Monitoring\n",
    "\n",
    "Welcome to Week 8. As we move towards production (Week 9), we must answer the most critical question: **\"How do we know if our model / agent is actually good?\"**\n",
    "\n",
    "###  Weekly Table of Contents\n",
    "1. [Building an LLM-as-a-Judge Pipeline](#-Lab-8.1:-Building-an-LLM-as-a-Judge-Pipeline)\n",
    "2. [Evaluating RAG with the \"RAG Triad\"](#-Lab-8.2:-Evaluating-RAG-with-the-\"RAG-Triad\")\n",
    "- LLM-as-a-Judge Logic\n",
    "- Automated Scoring with JSON Parsers\n",
    "- The RAG Triad: Faithfulness, Relevance, and Context\n",
    "\n",
    "###  Learning Objectives\n",
    "1.  Understand the concept of **LLM-as-a-Judge**.\n",
    "2.  Learn how to define evaluation metrics for RAG and Agents.\n",
    "3.  Use Gemini 1.5 Flash to evaluate the outputs of our local Ollama models.\n",
    "4.  Build an automated evaluation pipeline.\n",
    "\n",
    "---\n",
    "## 8.1 Why Evaluation is Hard\n",
    "\n",
    "Unlike traditional software, LLM outputs are non-deterministic and text-based. Checking for \"exact matches\" is useless. Instead, we use:\n",
    "*   **Benchmarks:** Static datasets (MMLU, GSM8K).\n",
    "*   **Human-in-the-loop:** Expensive and slow.\n",
    "*   **LLM Judges:** Using a superior model (Gemini 1.5 Flash) to grade a smaller model (Gemma).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ WEEK 8 INITIALIZATION\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# LANGCHAIN EVALUATION\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "load_dotenv(override=True)\n",
    "MODEL = \"gemini-1.5-flash\"\n",
    "LOCAL_MODEL = \"gemma2:2b\"\n",
    "\n",
    "# The Judge (Cloud)\n",
    "judge_llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0)\n",
    "\n",
    "# The Model being tested (Local or lower-capability)\n",
    "test_llm = Ollama(model=LOCAL_MODEL)\n",
    "\n",
    "print(f\"âœ… Week 8 Ready: Evaluation using {MODEL} as Judge.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648476d",
   "metadata": {},
   "source": [
    "##  Lab 8.1: Building an LLM-as-a-Judge Pipeline\n",
    "**Aim**: To establish an automated quality assurance workflow where a high-capability model (Gemini 1.5 Flash) assesses the performance of a local \"student\" model (Gemma 2).\n",
    "\n",
    "**Explanation**:\n",
    "This lab implements the \"LLM-as-a-Judge\" pattern:\n",
    "1.  **Metric Definition**: We define a 1-5 scale for \"Faithfulness\" and \"Accuracy.\"\n",
    "2.  **Structured Evaluation**: Using a `JsonOutputParser`, we force the judge to provide both a quantitative score and a qualitative reason.\n",
    "3.  **Benchmarking**: This allows developers to iterate on prompts or RAG settings and see a numerical improvement in model performance without manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an unbiased evaluator. Grade the 'Student Response' based on its 'Reference Facts'.\n",
    "Give a score from 1 to 5 (5 being perfectly accurate) and a brief reasoning.\n",
    "\n",
    "Reference Facts: {reference}\n",
    "Student Response: {response}\n",
    "\n",
    "Return your answer in the following JSON format:\n",
    "{{\n",
    "    \"score\": int,\n",
    "    \"reasoning\": \"string\"\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "eval_chain = eval_prompt | judge_llm | JsonOutputParser()\n",
    "\n",
    "# Test Case\n",
    "reference_fact = \"The capital of France is Paris. It has the Eiffel Tower.\"\n",
    "student_response = \"Paris is the capital of France and is known for the Eiffel Tower.\"\n",
    "\n",
    "print(\"Running LLM-as-a-Judge Evaluation...\")\n",
    "result = eval_chain.invoke({\"reference\": reference_fact, \"response\": student_response})\n",
    "print(f\"Score: {result['score']}/5\")\n",
    "print(f\"Reasoning: {result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d38bdd",
   "metadata": {},
   "source": [
    "##  Lab 8.2: Evaluating RAG with the \"RAG Triad\"\n",
    "**Aim**: To implement the industry-standard \"RAG Triad\" evaluation framework to identify specific points of failure in a retrieval-augmented generation system.\n",
    "\n",
    "**Explanation**:\n",
    "We break down RAG performance into three distinct components:\n",
    "1.  **Context Relevance**: Checks if the retriever found the right information.\n",
    "2.  **Faithfulness**: Ensures the generator didn't hallucinate or add outside knowledge.\n",
    "3.  **Answer Relevance**: Verifies that the final output actually answers the user's specific question.\n",
    "By measuring these separately, we can determine whether to fix the \"Retrieval\" (Vector DB) or the \"Generation\" (Prompting/Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d26856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance(question, context):\n",
    "    \"\"\"\n",
    "    Measures Context Relevance (Part of the RAG Triad)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    On a scale of 0 to 1, how relevant is the context below to the question? \n",
    "    Return ONLY a numerical value.\n",
    "    \n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Relevance Score:\"\"\"\n",
    "    \n",
    "    response = judge_llm.invoke(prompt)\n",
    "    try:\n",
    "        # Extracting numerical value from potential text\n",
    "        score_text = response.content.strip()\n",
    "        score = float(score_text)\n",
    "        return score\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Example Scenario\n",
    "test_question = \"What is the speed of light?\"\n",
    "test_context = \"The speed of light in a vacuum is exactly 299,792,458 metres per second.\"\n",
    "\n",
    "print(f\"Testing RAG Triad - Context Relevance...\")\n",
    "score = calculate_relevance(test_question, test_context)\n",
    "print(f\"Context Relevance Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e83aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and completion\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WEEK 8 COMPILED NOTEBOOK - SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"âœ… Evaluation environment initialized using Gemini 1.5 Flash as Judge\")\n",
    "print(\"âœ… Lab 8.1: LLM-as-a-Judge pipeline with JSON parsing ready\")\n",
    "print(\"âœ… Lab 8.2: RAG Triad evaluation logic ready\")\n",
    "print()\n",
    "print(\"ðŸš€ Ready to scientifically measure model performance!\")\n",
    "print(\"ðŸ“Š Use eval_chain.invoke() to test your student models.\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9496734",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Instructor's Evaluation & Lab Summary\n",
    "\n",
    "###  Assessment Criteria\n",
    "1. **Technical Implementation**: Adherence to the lab objectives and code functionality.\n",
    "2. **Logic & Reasoning**: Clarity in the explanation of the underlying AI principles.\n",
    "3. **Best Practices**: Use of secure environment variables and structured prompts.\n",
    "\n",
    "**Lab Completion Status: Verified**\n",
    "**Focus Area**: Language Modelling & Deep Learning Systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
