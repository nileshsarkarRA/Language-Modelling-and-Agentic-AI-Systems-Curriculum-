{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e800152d",
   "metadata": {},
   "source": [
    "## Week 6 Lab Manual\n",
    "### Foundations of Deep Learning & AI Functionality\n",
    "\n",
    "**Instructor Note**: This lab manual provides the aim, code, and explanation for each practical task. Focus on the architectural patterns and the transition from theoretical concepts to functional AI implementations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8907f",
   "metadata": {},
   "source": [
    "# Week 6: Local LLM Deep Dive & Quantization\n",
    "\n",
    "Welcome to Week 6. After exploring RAG systems, we now focus on the engine that powers many of these systems locally: **Ollama** and the concept of **Quantization**.\n",
    "\n",
    "###  Weekly Table of Contents\n",
    "1. [Advanced Local Control with Ollama SDK](#-Lab-6.1:-Advanced-Local-Control-with-Ollama-SDK)\n",
    "2. [Local LLM Benchmarking (Speed vs Model Size)](#-Lab-6.2:-Local-LLM-Benchmarking-(Speed-vs-Model-Size))\n",
    "3. [Understanding Quantization Levels & Modelfiles](#-Lab-6.3:-Understanding-Quantization-Levels-&-Modelfiles)\n",
    "\n",
    "###  Learning Objectives\n",
    "1.  Understand the benefits and trade-offs of Local vs Cloud LLMs.\n",
    "2.  Learn the basics of LLM Quantization (4-bit, 8-bit).\n",
    "3.  Master the Ollama Python SDK for advanced control.\n",
    "4.  Build a local benchmarking tool to measure tokens-per-second (TPS).\n",
    "\n",
    "---\n",
    "## 6.1 Local LLMs & The VRAM Problem\n",
    "\n",
    "Cloud models (like Gemini) have trillions of parameters and require massive server farms. To run these on a consumer laptop, we use:\n",
    "*   **Smaller Architectures:** Like Gemma 2 (2B or 9B parameters).\n",
    "*   **Quantization:** Reducing the precision of weights to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089224c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ WEEK 6 INITIALIZATION\n",
    "import os\n",
    "import time\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "load_dotenv(override=True)\n",
    "LOCAL_MODEL = \"gemma2:2b\" # Updated to gemma2 for better performance\n",
    "\n",
    "# Help with Ollama connection\n",
    "try:\n",
    "    ollama.list()\n",
    "    print(f\"‚úÖ Ollama is running.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Ollama is not running. Please start the Ollama application.\")\n",
    "\n",
    "print(f\"‚úÖ Week 6 Ready. Local Model: {LOCAL_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f2d81",
   "metadata": {},
   "source": [
    "##  Lab 6.1: Advanced Local Control with Ollama SDK\n",
    "**Aim**: To programmatically interact with local LLMs using the Ollama Python SDK, enabling streaming responses and custom system instructions.\n",
    "\n",
    "**Explanation**:\n",
    "This lab demonstrates how to move beyond basic Ollama CLI commands:\n",
    "1.  **SDK Integration**: We use the `ollama` Python library to initiate chat sessions directly from code.\n",
    "2.  **Streaming**: Implementing `stream=True` to improve perceived latency by displaying tokens as they are generated.\n",
    "3.  **Custom Modelfiles**: Showing how to create new model variants (e.g., `expert-gemma`) with baked-in system instructions and temperature parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom system prompt for a technical expert\n",
    "system_prompt = \"You are a senior C++ engineer. Give concise, low-level answers.\"\n",
    "\n",
    "def chat_stream(prompt):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    \n",
    "    response = ollama.chat(model=LOCAL_MODEL, messages=messages, stream=True)\n",
    "    \n",
    "    for chunk in response:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "# Test the stream\n",
    "chat_stream(\"How do I optimize a for loop for vectorization?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7aedd",
   "metadata": {},
   "source": [
    "##  Lab 6.2: Local LLM Benchmarking (Speed vs Model Size)\n",
    "**Aim**: To build a diagnostic tool that measures the performance of local LLMs in terms of Tokens Per Second (TPS) and response latency.\n",
    "\n",
    "**Explanation**:\n",
    "Performance metrics are critical for local deployment:\n",
    "1.  **Timer Logic**: We capture `start_time` and `end_time` around the generation call.\n",
    "2.  **Token Counting**: Utilizing Ollama's `eval_count` metadata to get the exact number of tokens generated.\n",
    "3.  **Efficiency Analysis**: By running this across different models (2B vs 7B), students can visualize the direct correlation between parameter count and inference speed on their specific hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model_name, prompt):\n",
    "    print(f\"\\nBenchmarking {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(model=model_name, prompt=prompt)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        tokens = response['eval_count']\n",
    "        tps = tokens / total_time\n",
    "        \n",
    "        print(f\"Total Time: {total_time:.2f}s\")\n",
    "        print(f\"Total Tokens: {tokens}\")\n",
    "        print(f\"Tokens Per Second: {tps:.2f} tokens/s\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error benchmarking {model_name}: {e}. Ensure the model is downloaded.\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_model(LOCAL_MODEL, 'Explain quantum physics in one paragraph.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c7066",
   "metadata": {},
   "source": [
    "##  Lab 6.3: Understanding Quantization Levels & Modelfiles\n",
    "**Aim**: To understand how model weight precision impacts performance and how to create custom local model instances using Ollama Modelfiles.\n",
    "\n",
    "**Explanation**:\n",
    "Ollama allows us to customize model behavior permanently:\n",
    "1.  **Quantization**: We discuss the math behind FP32 vs INT4. A 4-bit model typically offers the best balance for consumer hardware.\n",
    "2.  **Modelfiles**: Similar to Dockerfiles, these allow us to set parameters like `temperature` and `system_prompt` so they are baked into a new model name (e.g., `expert-gemma`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3bfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Programmatic Modelfile Creation\n",
    "# We will create a model called 'expert-gemma' that is specifically tuned for system administration questions.\n",
    "\n",
    "modelfile = f\"\"\"\n",
    "FROM {LOCAL_MODEL}\n",
    "PARAMETER temperature 0.2\n",
    "SYSTEM You are an expert Linux System Administrator. Answer only with technical commands.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Creating custom model 'expert-gemma'...\")\n",
    "# ollama.create(model='expert-gemma', modelfile=modelfile)\n",
    "\n",
    "# Test the custom model\n",
    "print(\"\\nTesting 'expert-gemma':\")\n",
    "# response = ollama.generate(model='expert-gemma', prompt=\"How do I check open ports on Ubuntu?\")\n",
    "# print(response['response'])\n",
    "print(\"Note: ollama.create is commented out to prevent repeated model creation during walkthroughs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156def52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and completion\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WEEK 6 COMPILED NOTEBOOK - SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"‚úÖ Local LLM environment configured\")\n",
    "print(\"‚úÖ Lab 6.1: Streaming SDK control ready\")\n",
    "print(\"‚úÖ Lab 6.2: Performance benchmarking tool ready\")\n",
    "print(\"‚úÖ Lab 6.3: Custom Modelfile generation logic ready\")\n",
    "print()\n",
    "print(\"üöÄ Ready to explore local inference and quantization!\")\n",
    "print(\"üìä Use benchmark_model() to compare different local models.\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620473ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Instructor's Evaluation & Lab Summary\n",
    "\n",
    "###  Assessment Criteria\n",
    "1. **Technical Implementation**: Adherence to the lab objectives and code functionality.\n",
    "2. **Logic & Reasoning**: Clarity in the explanation of the underlying AI principles.\n",
    "3. **Best Practices**: Use of secure environment variables and structured prompts.\n",
    "\n",
    "**Lab Completion Status: Verified**\n",
    "**Focus Area**: Language Modelling & Deep Learning Systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
