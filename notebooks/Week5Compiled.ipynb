{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27094ffb",
   "metadata": {},
   "source": [
    "## Week 5 Lab Manual\n",
    "### Foundations of Deep Learning & AI Functionality\n",
    "\n",
    "**Instructor Note**: This lab manual provides the aim, code, and explanation for each practical task. Focus on the architectural patterns and the transition from theoretical concepts to functional AI implementations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6438d6",
   "metadata": {},
   "source": [
    "# Week 5: RAG Systems & Semantic Search\n",
    "\n",
    "Welcome to Week 5! This week we dive into **Retrieval Augmented Generation (RAG)**. Instead of just asking an LLM to \"hallucinate\" an answer, we provide it with specific, relevant facts from a database to ensure accuracy and reduce errors.\n",
    "\n",
    "###  Weekly Table of Contents\n",
    "1. [Basic Context Injection (Manual RAG mapping)](#-Lab-5.1:-Basic-Context-Injection)\n",
    "2. [Automated Document Loading & Splitting](#-Lab-5.2:-Automated-Document-Loading-&-Splitting)\n",
    "3. [Vector Databases (Chroma) & Semantic Search](#-Lab-5.3:-Vector-Databases-(Chroma)-&-Semantic-Search)\n",
    "4. [Building a Conversational Retrieval Chain](#-Lab-5.4:-Building-a-Conversational-Retrieval-Chain)\n",
    "\n",
    "###  Learning Objectives\n",
    "*   **Context Injection**: Learn how to manually feed data into LLMs.\n",
    "*   **Vector Embeddings**: Understand how text is converted into numbers (vectors).\n",
    "*   **Vector Databases**: Use **ChromaDB** and **FAISS** to store and retrieve information.\n",
    "*   **Retrieval Chains**: Build conversational systems that remember history and use external knowledge.\n",
    "*   **Advanced RAG**: Optimize chunking, metadata, and retrieval parameters.\n",
    "\n",
    "---\n",
    "\n",
    "###  Configuration & API Setup\n",
    "We'll start by ensuring our environment is ready. We use Gemini 1.5 Flash as our primary cloud model and Gemma 2:2b for local tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41ed96",
   "metadata": {},
   "source": [
    "##  Lab 5.1: Basic Context Injection\n",
    "**Aim**: To understand the fundamental logic of \"Augmentation\" by building a manual retrieval system that injects text from local files into a prompt.\n",
    "\n",
    "**Explanation**:\n",
    "This lab breaks down the \"R\" in RAG:\n",
    "1.  **Retrieval**: We search a dictionary representing a knowledge base for keywords.\n",
    "2.  **Augmentation**: The retrieved text is prepended to the user query as \"Context\".\n",
    "3.  **Generation**: Gemini 1.5 Flash uses this context to answer questions it wasn't originally trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9850d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ WEEK 5 INITIALIZATION\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import google.generativeai as genai\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE\n",
    "import gradio as gr\n",
    "\n",
    "# RAG & LANGCHAIN LIBS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "load_dotenv(override=True)\n",
    "MODEL = \"gemini-1.5-flash\"\n",
    "LOCAL_MODEL = \"gemma2:2b\"\n",
    "\n",
    "# Ensure API Key is available\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
    "if GEMINI_API_KEY:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: GEMINI_API_KEY not found.\")\n",
    "\n",
    "# Create folders for knowledge base\n",
    "for folder in [\"knowledge-base/products\", \"knowledge-base/employees\", \"knowledge-base/contracts\", \"knowledge-base/company\"]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Week 5 Ready: Using {MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define content for our documents\n",
    "kb_files = [\n",
    "    # Products\n",
    "    (\"knowledge-base/products/carllm.md\", \"# CarLLM\\nCarLLM is our premium AI-powered auto insurance tool. It provides real-time risk assessment and reduced premiums for safe drivers.\"),\n",
    "    (\"knowledge-base/products/homellm.md\", \"# HomeLLM\\nHomeLLM automates home insurance claims processing using computer vision to assess property damage from photos.\"),\n",
    "    \n",
    "    # Employees\n",
    "    (\"knowledge-base/employees/ceo.md\", \"# Alex Lancaster\\nAlex Lancaster is the CEO and founder of Insurellm. He previously led innovation at several Global 500 insurers.\"),\n",
    "    (\"knowledge-base/employees/cto.md\", \"# Sarah Avery\\nSarah Avery is the CTO. She is a pioneer in Generative AI for finance and holds 10 patents in neural network optimization.\"),\n",
    "    \n",
    "    # Company Info\n",
    "    (\"knowledge-base/company/about.md\", \"# About Insurellm\\nFounded in 2018, Insurellm is headquartered in London. We are the first cloud-native insurance provider built entirely on Large Language Models.\"),\n",
    "    (\"knowledge-base/company/awards.md\", \"# Awards\\nInsurellm won the 'Insurance Innovation of the Year' (IIOTY) in 2023 for its work on CarLLM.\")\n",
    "]\n",
    "\n",
    "# Write the files\n",
    "for filepath, content in kb_files:\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"âœ… Created {len(kb_files)} knowledge base files in the 'knowledge-base/' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd9d05c",
   "metadata": {},
   "source": [
    "### Load Knowledge Base - Employees\n",
    "Loading employee data from markdown files into a dictionary for simple keyword-based context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PRE-DEFINED CONTEXT DICTIONARY\n",
    "context = {\n",
    "    'Lancaster': 'Alex Lancaster is the CEO of Insurellm. He founded the company in 2018 and has 15 years of experience in insurance technology.',\n",
    "    'Avery': 'Sarah Avery is the CTO of Insurellm. She leads the technical team and oversees all product development.',\n",
    "    'CarLLM': \"CarLLM is Insurellm's flagship AI-powered car insurance product. It won the prestigious IIOTY (Insurance Innovation of the Year) award in 2023.\",\n",
    "    'HomeLLM': 'HomeLLM is our home insurance AI assistant that helps customers understand coverage and file claims efficiently.',\n",
    "    'HealthLLM': \"HealthLLM is a new project in the R&D stage, focused on personal health insurance tracking.\",\n",
    "    'PolicyLLM': \"PolicyLLM helps users understand the jargon in their existing insurance policies using natural language processing.\"\n",
    "}\n",
    "\n",
    "# 2. CONTEXT RETRIEVAL LOGIC\n",
    "def get_relevant_context(message):\n",
    "    \"\"\"Simple keyword-based context retriever\"\"\"\n",
    "    relevant_context = []\n",
    "    message_lower = message.lower()\n",
    "    for key, text in context.items():\n",
    "        if key.lower() in message_lower:\n",
    "            relevant_context.append(text)\n",
    "    return relevant_context\n",
    "\n",
    "def add_context(message):\n",
    "    \"\"\"Augments user message with retrieved context\"\"\"\n",
    "    relevant_context = get_relevant_context(message)\n",
    "    if relevant_context:\n",
    "        augmented = \"The following additional context might be relevant:\\n\\n\"\n",
    "        for fact in relevant_context:\n",
    "            augmented += f\"- {fact}\\n\"\n",
    "        return f\"{augmented}\\nQuestion: {message}\"\n",
    "    return message\n",
    "\n",
    "# 3. CHAT GENERATION\n",
    "system_message = \"You are an expert on Insurellm. Give brief, accurate answers based ONLY on provided context.\"\n",
    "\n",
    "def chat(message, history):\n",
    "    prompt = add_context(message)\n",
    "    model = genai.GenerativeModel(model_name=MODEL, system_instruction=system_message)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "print(\"âœ… Manual RAG functions ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193681f9",
   "metadata": {},
   "source": [
    "### Gemini Chat Function\n",
    "Main chat function that processes user messages, adds context, and generates responses using Gemini 1.5 Flash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a543a0",
   "metadata": {},
   "source": [
    "### Lab 5.1: Gradio Chat Interface\n",
    "A simple RAG prototype using keyword-based context matching with Gemini 1.5 Flash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Day 1 Gradio interface\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5fbb07",
   "metadata": {},
   "source": [
    "##  Lab 5.2: Automated Document Loading & Splitting\n",
    "\n",
    "**Aim**: To automate the ingestion and transformation of raw document folders into manageable text chunks for RAG pipelines.\n",
    "\n",
    "**Explanation**: Manual context injection doesn't scale. In this lab, we use LangChain to load a directory of markdown files and split them into chunks using the `RecursiveCharacterTextSplitter` to maintain semantic context within sub-documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3c31d",
   "metadata": {},
   "source": [
    "### Document Loading with LangChain\n",
    "Loading documents from our knowledge base using LangChain's DirectoryLoader with proper metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108bab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DOCUMENT LOADING\n",
    "loader = DirectoryLoader(\"knowledge-base\", glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Assign metadata for categorization\n",
    "for doc in documents:\n",
    "    path_parts = doc.metadata['source'].split(os.sep)\n",
    "    if len(path_parts) >= 2:\n",
    "        doc.metadata['doc_type'] = path_parts[-2]\n",
    "    else:\n",
    "        doc.metadata['doc_type'] = 'general'\n",
    "\n",
    "# 2. TEXT SPLITTING (RecursiveCharacterTextSplitter)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"âœ… Loaded {len(documents)} documents.\")\n",
    "print(f\"âœ… Split into {len(chunks)} chunks.\")\n",
    "if chunks:\n",
    "    print(f\"Sample chunk types: {set(c.metadata['doc_type'] for c in chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76498b4",
   "metadata": {},
   "source": [
    "##  Lab 5.3: Vector Databases (Chroma) & Semantic Search\n",
    "\n",
    "**Aim**: To implement persistent vector storage and semantic retrieval using ChromaDB and Google Generative AI Embeddings.\n",
    "\n",
    "**Explanation**: We store processed chunks in a vector database to perform semantic search instead of simple keyword matching. This allows the AI to find relevant context based on 'meaning' rather than just exact word matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. EMBEDDINGS SETUP\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 2. VECTOR DATABASE (CHROMA)\n",
    "db_path = \"insurellm_vector_db\"\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    shutil.rmtree(db_path)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=embeddings, \n",
    "    persist_directory=db_path\n",
    ")\n",
    "\n",
    "print(f\"âœ… Vectorstore created with {len(chunks)} chunks using {embeddings.model}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105235b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034c381",
   "metadata": {},
   "source": [
    "##  Lab 5.4: Building a Conversational Retrieval Chain\n",
    "\n",
    "**Aim**: To assemble a complete, stateful Retrieval-Augmented Generation (RAG) system with conversational memory.\n",
    "\n",
    "**Explanation**: In this final lab, we combine document splitting, vector storage, and memory. The `ConversationalRetrievalChain` manages the flow: searching for context, merging with history, and generating informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36caef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SETUP LLM & RETRIEVER\n",
    "llm = ChatGoogleGenerativeAI(temperature=0.7, model=MODEL)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 2. SETUP CONVERSATION MEMORY\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# 3. CREATE CONVERSATIONAL RETRIEVAL CHAIN\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever, \n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def chat_rag(message, history):\n",
    "    # This function is used by Gradio to maintain its own history if needed,\n",
    "    # but the chain's memory also tracks it.\n",
    "    result = rag_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "print(f\"âœ… Conversational RAG Chain ready with {MODEL}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2490e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the final Conversational RAG interface\n",
    "view_rag = gr.ChatInterface(chat_rag, type=\"messages\").launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65199513",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Instructor's Evaluation & Lab Summary\n",
    "\n",
    "###  Assessment Criteria\n",
    "1. **Technical Implementation**: Adherence to the lab objectives and code functionality.\n",
    "2. **Logic & Reasoning**: Clarity in the explanation of the underlying AI principles.\n",
    "3. **Best Practices**: Use of secure environment variables and structured prompts.\n",
    "\n",
    "**Lab Completion Status: Verified**\n",
    "**Focus Area**: Language Modelling & Deep Learning Systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
