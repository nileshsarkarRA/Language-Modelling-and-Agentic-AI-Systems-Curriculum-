{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ee4956",
   "metadata": {},
   "source": [
    "## Week 9 Lab Manual\n",
    "### Foundations of Deep Learning & AI Functionality\n",
    "\n",
    "**Instructor Note**: This lab manual provides the aim, code, and explanation for each practical task. Focus on the architectural patterns and the transition from theoretical concepts to functional AI implementations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1908f51",
   "metadata": {},
   "source": [
    "# Week 9: Model Specialization, Production & Capstone Project\n",
    "## From Notebooks to Production-Ready AI Systems\n",
    "\n",
    "###  Weekly Table of Contents\n",
    "1. [Specialized Dataset Lab](#-Lab-9.1:-Specialized-Dataset-Lab)\n",
    "2. [Production Streaming API](#-Lab-9.2:-Production-Streaming-API)\n",
    "3. [Capstone Project: The Autonomous Multi-Modal Researcher](#-Lab-9.3:-Capstone-Project--The-Autonomous-Multi-Modal-Researcher)\n",
    "- Synthetic Data Generation for Fine-Tuning\n",
    "- FastAPI Integration with Server-Sent Events (SSE)\n",
    "- Real-time Performance Optimization\n",
    "\n",
    "###  Learning Objectives\n",
    "This final week focuses on transitioning from a \"working prototype\" to a \"production system.\" You will cover:\n",
    "1.  **Advanced Model Tuning**: Deep dive into PEFT, LoRA, and the logic of Synthetic Data Generation.\n",
    "2.  **Streaming & UI Optimization**: Implementing real-time feedback loops for better user experience.\n",
    "3.  **Production Architectures**: Deploying LLM logic via **FastAPI** with background tasks and streaming.\n",
    "4.  **Capstone Integration**: Building a unified \"Autonomous Researcher\" agent.\n",
    "\n",
    "---\n",
    "\n",
    "###  9.1 Theory: Fine-Tuning vs. RAG (The Final Verdict)\n",
    "By now, you've seen RAG (Week 5) and Prompt Engineering (Week 4). Why bother with Fine-Tuning?\n",
    "\n",
    "| Feature | Prompting / RAG | Fine-Tuning |\n",
    "| :--- | :--- | :--- |\n",
    "| **Knowledge Update** | Easy (Update Vector DB) | Hard (Requires retraining) |\n",
    "| **New Task Adaptation** | Good | Excellent |\n",
    "| **Cost to Latency** | High (Long context) | Low (Shorter prompts) |\n",
    "| **Steerability** | Variable | Very High (Consistent tone/format) |\n",
    "\n",
    "**Modern Convergence**: Most production systems use a **Hybrid Approach**:\n",
    "- Fine-tune for **style, format, and reasoning**.\n",
    "- RAG for **up-to-date facts and data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34670f1",
   "metadata": {},
   "source": [
    "##  Lab 9.1: Specialized Dataset Lab\n",
    "**Aim**: To generate a high-quality, synthetic training dataset using a \"Teacher\" model (Gemini 1.5 Flash) to prepare a smaller \"Student\" model (Gemma 2) for specialized tasks through fine-tuning.\n",
    "\n",
    "**Explanation**:\n",
    "This lab demonstrates the first step in the fine-tuning pipeline:\n",
    "1.  **Structured Generation**: We use `Pydantic` to ensure the teacher model returns perfectly formatted JSON training pairs.\n",
    "2.  **Specialization**: By providing industry-specific instructions (e.g., Legal Contract Analysis), we generate data that captures professional jargon and complex risk assessment patterns.\n",
    "3.  **JSONL Export**: The output is saved in `.jsonl` format, the standard for training modern LLMs via techniques like LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b082a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ WEEK 9 INITIALIZATION\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Standardized Model Definitions\n",
    "MODEL = \"gemini-1.5-flash\"\n",
    "LOCAL_MODEL = \"gemma2:2b\"\n",
    "\n",
    "# API Clients\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Initialize Teacher Model\n",
    "llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.7)\n",
    "\n",
    "# --- Lab 9.1: Specialized Dataset Lab ---\n",
    "\n",
    "# Define the structure of our training data\n",
    "class TrainingExample(BaseModel):\n",
    "    instruction: str = Field(description=\"The user query or prompt\")\n",
    "    response: str = Field(description=\"The ideal expert assistant response\")\n",
    "\n",
    "class Dataset(BaseModel):\n",
    "    examples: List[TrainingExample]\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Dataset)\n",
    "\n",
    "# Generation Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Generate 5 diverse and high-quality training examples for a Legal Contract Analyzer.\\n{format_instructions}\\nEnsure the language is professional, contains legal terminology, and focuses on risk identification.\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(\"ðŸš€ Generating synthetic legal dataset...\")\n",
    "# Note: Limit to 5 for the lab demonstration speed\n",
    "result = chain.invoke({})\n",
    "\n",
    "# Export to JSONL for Fine-Tuning\n",
    "output_filename = \"legal_training_data.jsonl\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    for ex in result[\"examples\"]:\n",
    "        # Standard Alpaca/Gemma format: {\"instruction\": \"...\", \"response\": \"...\"}\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Successfully created {len(result['examples'])} training pairs in {output_filename}\")\n",
    "print(\"\\n--- Preview of Example 1 ---\")\n",
    "print(f\"Q: {result['examples'][0]['instruction'][:80]}...\")\n",
    "print(f\"A: {result['examples'][0]['response'][:80]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df903c",
   "metadata": {},
   "source": [
    "##  Lab 9.2: Production Streaming API\n",
    "**Aim**: To build a production-ready, asynchronous API service using FastAPI that supports token-by-token streaming for real-time user experiences.\n",
    "\n",
    "**Explanation**:\n",
    "This lab focuses on the engineering requirements of deployment:\n",
    "1.  **FastAPI Integration**: We create a RESTful endpoint that accepts user prompts and returns a `StreamingResponse`.\n",
    "2.  **SSE (Server-Sent Events)**: The backend uses Python generators to yield tokens as they arrive from the LLM, keeping the HTTP connection open until the response is complete.\n",
    "3.  **Latency Optimization**: By streaming results, we minimize the \"Time To First Token\" (TTFT), significantly improving the perceived speed of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ddc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lab 9.2: Production Streaming API ---\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "import ollama\n",
    "import uvicorn\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    prompt: str\n",
    "    stream: bool = True\n",
    "    provider: str = \"cloud\" # \"cloud\" or \"local\"\n",
    "\n",
    "async def generate_gemini_stream(prompt):\n",
    "    # Using the standardized Gemini 1.5 Flash\n",
    "    llm = ChatGoogleGenerativeAI(model=MODEL) \n",
    "    async for chunk in llm.astream(prompt):\n",
    "        yield f\"data: {chunk.content}\\n\\n\"\n",
    "    yield \"data: [DONE]\\n\\n\"\n",
    "\n",
    "async def generate_ollama_stream(prompt):\n",
    "    # Using the standardized local model Gemma 2\n",
    "    stream = ollama.generate(model=LOCAL_MODEL, prompt=prompt, stream=True)\n",
    "    for chunk in stream:\n",
    "        if 'response' in chunk:\n",
    "            yield f\"data: {chunk['response']}\\n\\n\"\n",
    "    yield \"data: [DONE]\\n\\n\"\n",
    "\n",
    "@app.post(\"/chat/stream\")\n",
    "async def chat_stream(request: ChatRequest):\n",
    "    \"\"\"\n",
    "    Production-ready endpoint for real-time LLM interaction\n",
    "    \"\"\"\n",
    "    if request.provider == \"cloud\":\n",
    "        return StreamingResponse(generate_gemini_stream(request.prompt), media_type=\"text/event-stream\")\n",
    "    else:\n",
    "        return StreamingResponse(generate_ollama_stream(request.prompt), media_type=\"text/event-stream\")\n",
    "\n",
    "print(\"ðŸ”Œ Streaming API Service Defined.\")\n",
    "print(f\"Cloud Model: {MODEL}\")\n",
    "print(f\"Local Model: {LOCAL_MODEL}\")\n",
    "print(\"\\nInstructions to run this service:\")\n",
    "print(\"1. Save this code to a file named 'main.py'\")\n",
    "print(\"2. Run 'uvicorn main:app --reload' in your terminal\")\n",
    "print(\"3. Test with: curl -X POST http://localhost:8000/chat/stream -d '{\\\"prompt\\\": \\\"Hello\\\"}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d69277d",
   "metadata": {},
   "source": [
    "##  Lab 9.3: Capstone Project  The Autonomous Multi-Modal Researcher\n",
    "\n",
    "**Aim**: To architect and deploy a production-ready, agentic system that integrates RAG, LangGraph-based reasoning, and multi-modal fallback capabilities.\n",
    "\n",
    "**Explanation**:\n",
    "This capstone is the culmination of the curriculum, requiring the integration of:\n",
    "1. **Dynamic Ingestion**: Automated monitoring and parsing of document folders.\n",
    "2. **Agentic Reasoning**: A LangGraph state machine that decides between retrieval and web research.\n",
    "3. **Production APIs**: Deployment via FastAPI and Gradio with real-time thought visualization.\n",
    "4. **Automated Evaluation**: A separate model acting as a judge to verify the accuracy and completeness of generated reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c313b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Instructor's Evaluation & Lab Summary\n",
    "\n",
    "###  Assessment Criteria\n",
    "1. **Technical Implementation**: Adherence to the lab objectives and code functionality.\n",
    "2. **Logic & Reasoning**: Clarity in the explanation of the underlying AI principles.\n",
    "3. **Best Practices**: Use of secure environment variables and structured prompts.\n",
    "\n",
    "**Lab Completion Status: Verified**\n",
    "**Focus Area**: Language Modelling & Deep Learning Systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
